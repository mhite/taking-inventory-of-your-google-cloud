# Taking Inventory of Your Google Cloud

Splunk Cloud Architect [Paul Davies](https://github.com/pauld-splunk) recently authored and released the [GCP Application Template](https://splunkbase.splunk.com/app/5404/), a blueprint of visualizations, reports, and searches focused on Google Cloud use cases. Many of the reports included in his application require Google Cloud [asset inventory](https://cloud.google.com/asset-inventory) data to be periodically generated and sent into Splunk. But *how exactly* do you craft that inventory generation pipeline so you can "light-up" Paul's application dashboards and reports?

In this blog post, I'll describe and compare three methods operators can use to ingest Google Cloud asset inventory data  into Splunk. The first method leverages a "pull" data ingest strategy while the other two methods I cover are "push" based. For each ingest method, I'll provide detailed setup instructions or provide pointers to them. Finally, I'll make a personal recommendation on what method I would choose if it was my own environment.

## Note: Export vs. Feed

Google provides both a [batch export](https://cloud.google.com/asset-inventory/docs/exporting-to-cloud-storage) and a [change feed](https://cloud.google.com/asset-inventory/docs/monitoring-asset-changes) view of asset data. The GCP Application Template leverages data generated by the [batch export data API](https://cloud.google.com/asset-inventory/docs/reference/rest/v1p7beta1/TopLevel/exportAssets) and does not support the [TemporalAsset](https://cloud.google.com/asset-inventory/docs/reference/rpc/google.cloud.asset.v1#temporalasset) schema used in the [asset change feed API](https://cloud.google.com/asset-inventory/docs/reference/rpc/google.cloud.asset.v1#createfeedrequest).

## Pull

### Method #1 - Pull from bucket

The first method involves using Cloud Scheduler to trigger a Cloud Function on a regular schedule. This Cloud Function then sends a batch export request to the Asset Inventory API. This results in a bulk export of the current cloud asset inventory to a Cloud Storage bucket. Finally, the Splunk Add-on for Google Cloud Platform (GCP-TA) is configured to periodically monitor and ingest new files appearing in the Cloud Storage bucket. The following diagram illustrates this process.

![](images/asset-serverless-pull.png)

#### Components

* Cloud Scheduler
* Cloud Pub/Sub
* [Cloud Function (asset export)](https://github.com/splunk/splunk-gcp-functions/blob/master/Assets/main.py)
* Asset Inventory API
* Cloud Storage
* [GCP-TA storage input](https://docs.splunk.com/Documentation/AddOns/released/GoogleCloud/Configureinputsv4storagebuckets)

#### Pros

* Simple setup
* Pull solution avoids Splunk HEC whitelisting considerations
* GCP-TA is developed and supported by Splunk
* Minimal Google Cloud infrastructure cost to implement

#### Cons

* Large inventories may exceed file size limitations of GCP-TA GCS input (~250 MB)

#### Setup

##### Export Variables

Please export the following variables so they can be referenced in the setup steps below.

Note: Please ensure `GCP_PROJECT` is set to an existing project and `GCP_REGION` is set to your region of choice. Additionally, set `STUB` to a unique value.

```bash
export STUB=$(whoami)
export GCP_PROJECT=<your-project-id>
export GCP_REGION=us-central1
export ASSET_BUCKET=${STUB}-asset-bucket
export PUBSUB_FUNCTION=AssetExportPubSubFunction
export PUBSUB_TOPIC=asset-export-trigger-topic
export CS_JOB_NAME=asset-export-to-gcs-job
export CF_SERVICE_ACCOUNT=asset-export-cf-sa
export CF_SERVICE_ACCOUNT_EMAIL=${CF_SERVICE_ACCOUNT}@${GCP_PROJECT}.iam.gserviceaccount.com
export GCP_PROJECT_NUMBER=$(gcloud projects describe $GCP_PROJECT --format="value(projectNumber)")
```

##### Create Bucket

Create a Cloud Storage bucket to receive and hold exported asset inventory data.

```bash
gsutil mb gs://${ASSET_BUCKET}
```

Auto-expunge inventory files after 30 days.

```bash
cat << EOF > lifecycle.json
{
"lifecycle": {
  "rule": [
  {
    "action": {"type": "Delete"},
    "condition": {
      "age": 30,
      "isLive": true
    }
  }
]
}
}
EOF
gsutil lifecycle set lifecycle.json gs://${ASSET_BUCKET}
```

##### Create Pub/Sub Topic

Create a Pub/Sub topic that Cloud Scheduler will use to publish event triggers for the Cloud Function.

```bash
gcloud pubsub topics create $PUBSUB_TOPIC
```

##### Create Service Account

Create a service account for the asset export Cloud Function.

```bash
gcloud iam service-accounts create ${CF_SERVICE_ACCOUNT}
```

##### Service Account Permissions

Grant the new service account the project-wide `roles/cloudasset.viewer` IAM role. This will allow the Cloud Function permissions to trigger an asset inventory export.

```bash
gcloud projects add-iam-policy-binding ${GCP_PROJECT_NUMBER} \
    --member=serviceAccount:${CF_SERVICE_ACCOUNT_EMAIL} \
    --role=roles/cloudasset.viewer
```

##### Deploy Cloud Function

Save and deploy the asset export Cloud Function.

```bash
mkdir ${PUBSUB_FUNCTION} && cd $_
cat << EOF > main.py
import os
import time


def hello_pubsub(event, context):

    from google.cloud import asset_v1

    parent_id = os.environ["PARENT"]

    dump_file_path = os.environ["GCS_FILE_PATH"]
    now = time.time()

    client = asset_v1.AssetServiceClient()
    output_config = asset_v1.OutputConfig()
    output_config.gcs_destination.uri = dump_file_path + str(now) + ".json"
    content_type = asset_v1.ContentType.RESOURCE

    response = client.export_assets(
        request={
            "parent": parent_id,
            "content_type": content_type,
            "output_config": output_config,
        }
    )
EOF
cat << EOF > requirements.txt
# Function dependencies
google-cloud-asset==2.1.0
EOF
gcloud functions deploy ${PUBSUB_FUNCTION} \
  --runtime=python37 \
  --ingress-settings=internal-only \
  --service-account=${CF_SERVICE_ACCOUNT_EMAIL} \
  --timeout=120 \
  --trigger-topic=${PUBSUB_TOPIC} \
  --entry-point=hello_pubsub \
  --set-env-vars=GCS_FILE_PATH=gs://${ASSET_BUCKET}/asset_file_,PARENT=projects/${GCP_PROJECT} \
  --region=${GCP_REGION}
```

##### Configure Cloud Scheduler

Configure the Cloud Scheduler to export an asset inventory every 24 hours.

```bash
gcloud scheduler jobs create pubsub ${CS_JOB_NAME} --schedule "0 0 * * *" --time-zone="Etc/UTC" --topic ${PUBSUB_TOPIC} --message-body "trigger" --location=${GCP_REGION}
```

##### Configure props.conf

You will need to add the following to the `[google:gcp:buckets:jsondata]` section of `$SPLUNK_HOME/etc/apps/Splunk_TA_google-cloudplatform/local/props.conf`:

```
TRANSFORMS-sourcetype_gcp_assets = sourcetype_gcp_assets
```

##### Configure transforms.conf

Similarly, the following addition is required in the `$SPLUNK_HOME/etc/apps/Splunk_TA_google-cloudplatform/local/transforms.conf` file:

```
[sourcetype_gcp_assets]
REGEX = \"asset_type\"\:
FORMAT = sourcetype::google:gcp:assets
DEST_KEY = MetaData:Sourcetype
```

Please note that this is in addition to the `props.conf` and `transforms.conf` [modifications required by the GCP Application Template](https://splunkbase.splunk.com/app/5404/#/details).

##### Configure GCP-TA storage input

Please refer to the "[Configure Cloud Storage Bucket inputs](https://docs.splunk.com/Documentation/AddOns/released/GoogleCloud/Configureinputsv4storagebuckets)" section of the [Splunk Add-on for Google Cloud Platform documentation](https://docs.splunk.com/Documentation/AddOns/released/GoogleCloud/About).

## Push

### Method #2 - Serverless push-to-Splunk

This method leverages Cloud Functions not only for triggering an export of asset inventory data, but also to perform the delivery to a Splunk HEC. Cloud Scheduler regularly triggers a Cloud Function which in turn is responsible for initiating an Asset Inventory API bulk export to Cloud Storage. A second Cloud Function is configured to trigger on bucket object create/finalize events. This function will split the exported files into smaller files if necessary and deliver them directly to a Splunk HEC. Should the delivery fail, the messages are placed into a Pub/Sub topic for later redelivery attempts. The following diagram illustrates this process.

![](./images/asset-serverless-push.png)

#### Components

* Cloud Scheduler
* [Cloud Function (asset export)](https://github.com/splunk/splunk-gcp-functions/blob/master/Assets/main.py)
* Asset Inventory API
* Cloud Storage
* [Cloud Function (Cloud Storage ingest)](https://github.com/splunk/splunk-gcp-functions/blob/master/GCS/main.py)
* Pub/Sub
* [Cloud Function (delivery retry)](https://github.com/splunk/splunk-gcp-functions/tree/master/Retry)
* [Splunk HEC](https://docs.splunk.com/Documentation/Splunk/8.1.3/Data/UsetheHTTPEventCollector)

#### Pros

* Solution should incur minimal Google Cloud infrastructure costs
* Automated redelivery attempts of deadletters
* GCP Application Template is primarily tested with this method as it is written by the same [author](https://github.com/pauld-splunk)

#### Cons

* [Not a Splunk or Google supported solution](https://github.com/splunk/splunk-gcp-functions/blob/master/README.md#support)
* Whitelisting Cloud Function to Splunk HEC traffic requires additional complexity of [Cloud NAT](https://cloud.google.com/nat/docs/using-nat) and [advanced Serverless VPC Access connector configuration](https://cloud.google.com/functions/docs/networking/network-settings#route-egress-to-vpc)

#### Setup

Details on how to configure this method can be found in the [splunk-gcp-functions GitHub repository](https://github.com/splunk/splunk-gcp-functions/tree/master/Assets).

### Method #3 - Dataflow

The final method leverages Dataflow batch and streaming jobs to facilitate delivery of asset inventory data to a Splunk HEC. This approach uses Cloud Scheduler to regularly trigger a Cloud Function which in turn is responsible for initiating an Asset Inventory API bulk export to Cloud Storage. Another Cloud Function receives an event trigger when the export operation is complete. This function then starts a [batch Dataflow job which converts newline-delimited JSON files into Pub/Sub messages](https://cloud.google.com/dataflow/docs/guides/templates/provided-batch#gcstexttocloudpubsub) and publishes them to a topic. In parallel, a [streaming Dataflow pipeline is also running](https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#cloudpubsubtosplunk) which subscribes to the aforementioned topic and delivers them to a Splunk HEC.

The following diagram illustrates this process.

![](images/asset-export-pipeline.png)

#### Components

* Cloud Scheduler
* Cloud Function (asset export)
* Asset Inventory API
* Cloud Storage
* Cloud Function (launch Dataflow batch job)
* [Batch Dataflow job](https://cloud.google.com/dataflow/docs/guides/templates/provided-batch#gcstexttocloudpubsub)
* Pub/Sub
* [Streaming Dataflow job](https://cloud.google.com/dataflow/docs/guides/templates/provided-batch#gcstexttocloudpubsub)
* [Splunk HEC](https://docs.splunk.com/Documentation/Splunk/8.1.3/Data/UsetheHTTPEventCollector)

#### Pros

* Dataflow templates developed and supported by Google
* Natural choice for environments already leveraging streaming Dataflow to Splunk HEC log delivery pipeline
* Dead letter topic support for undeliverable messages
* Easy to monitor delivery backlog through Google Cloud Pub/Sub metrics
* Encryption of HEC token using KMS

#### Cons

* Both Dataflow templates are in [beta, pre-GA launch stage](https://cloud.google.com/products#product-launch-stages)
* More complex setup required compared to other methods
* Additional costs of running a Dataflow cluster
* Dataflow worker to Splunk HEC whitelisting requires [Cloud NAT](https://cloud.google.com/nat/docs/using-nat) to ensure static egress IPs

#### Setup

This method builds upon the steps outlined in method #1 (pull from bucket). I am repeating them here for clarity.

Also, please note that these instructions assume you are already running a streaming Dataflow job to deliver your Cloud Logging events to a Splunk HEC. Refer to the [Deploying Production Ready Log Exports to Splunk using Dataflow](https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow) documentation for more information on configuring the Splunk Dataflow template.

##### Export Variables

Please export the following variables so they can be referenced in the setup steps below.

Note: Please ensure `GCP_PROJECT` is set to an existing project and `GCP_REGION` is set to your region of choice. Additionally, set `STUB` to a unique value. You will also need to supply the Pub/Sub topic to target for your streaming Dataflow job in the `DF_BATCH_OUTPUT_TOPIC` environment variable. This topic variable must be in the form `projects/<PROJECT-NAME>/topics/<TOPIC-NAME>`.

```bash
export STUB=$(whoami)
export GCP_PROJECT=<your-project-id>
export GCP_REGION=us-central1
export ASSET_BUCKET=${STUB}-asset-bucket
export DF_TEMP_BUCKET=${STUB}-df-tmp-bucket
export PUBSUB_FUNCTION=AssetExportPubSubFunction
export DF_FUNCTION=DataflowBatchLaunchFunction
export PUBSUB_TOPIC=asset-export-trigger-topic
export DF_BATCH_OUTPUT_TOPIC=<your deliver-to-hec topic>
export CS_JOB_NAME=asset-export-to-gcs-job
export CF_SERVICE_ACCOUNT=asset-export-cf-sa
export CF_SERVICE_ACCOUNT_EMAIL=${CF_SERVICE_ACCOUNT}@${GCP_PROJECT}.iam.gserviceaccount.com
export GCP_PROJECT_NUMBER=$(gcloud projects describe $GCP_PROJECT --format="value(projectNumber)")
```

##### Create Buckets

Create a Cloud Storage bucket to receive and hold exported asset inventory data.

```bash
gsutil mb gs://${ASSET_BUCKET}
```

Auto-expunge inventory files after 30 days.

```bash
cat << EOF > lifecycle.json
{
"lifecycle": {
  "rule": [
  {
    "action": {"type": "Delete"},
    "condition": {
      "age": 30,
      "isLive": true
    }
  }
]
}
}
EOF
gsutil lifecycle set lifecycle.json gs://${ASSET_BUCKET}
```

Create a Cloud Storage bucket for the batch Dataflow job.

```bash
gsutil mb gs://${DF_TEMP_BUCKET}
```

##### Create Pub/Sub Topic

Create a Pub/Sub topic that Cloud Scheduler will use to publish event triggers for the Cloud Function.

```bash
gcloud pubsub topics create $PUBSUB_TOPIC
```

##### Create Service Account

Create a service account for the asset export Cloud Function.

```bash
gcloud iam service-accounts create ${CF_SERVICE_ACCOUNT}
```

##### Service Account Permissions

Grant the new service account the project-wide `roles/cloudasset.viewer` IAM role. This will allow the Cloud Function permissions to trigger an asset inventory export.

```bash
gcloud projects add-iam-policy-binding ${GCP_PROJECT_NUMBER} \
    --member=serviceAccount:${CF_SERVICE_ACCOUNT_EMAIL} \
    --role=roles/cloudasset.viewer
```

##### Deploy Cloud Functions

Save and deploy the asset export Cloud Function.

```bash
mkdir ${PUBSUB_FUNCTION} && cd $_
cat << EOF > main.py
import os
import time


def hello_pubsub(event, context):

    from google.cloud import asset_v1

    parent_id = os.environ["PARENT"]

    dump_file_path = os.environ["GCS_FILE_PATH"]
    now = time.time()

    client = asset_v1.AssetServiceClient()
    output_config = asset_v1.OutputConfig()
    output_config.gcs_destination.uri = dump_file_path + str(now) + ".json"
    content_type = asset_v1.ContentType.RESOURCE

    response = client.export_assets(
        request={
            "parent": parent_id,
            "content_type": content_type,
            "output_config": output_config,
        }
    )
EOF
cat << EOF > requirements.txt
# Function dependencies
google-cloud-asset==2.1.0
EOF
gcloud functions deploy ${PUBSUB_FUNCTION} \
  --runtime=python37 \
  --ingress-settings=internal-only \
  --service-account=${CF_SERVICE_ACCOUNT_EMAIL} \
  --timeout=120 \
  --trigger-topic=${PUBSUB_TOPIC} \
  --entry-point=hello_pubsub \
  --set-env-vars=GCS_FILE_PATH=gs://${ASSET_BUCKET}/asset_file_,PARENT=projects/${GCP_PROJECT} \
  --region=${GCP_REGION}
cd ..
```

Save and deploy the batch Dataflow job launcher Cloud Function.

```bash
mkdir ${DF_FUNCTION} && cd $_
cat << EOF > main.py
import os

def hello_gcs(event, context):
    # avoid triggering on file touches from gcsfuse
    if "size" in event and int(event["size"]) > 0:

        from googleapiclient.discovery import build

        template = (
            os.environ.get("TEMPLATE_GCS_PATH")
            or "gs://dataflow-templates/latest/GCS_Text_to_Cloud_PubSub"
        )
        temp_location = os.environ["GCS_TEMP_PATH"]
        project = os.environ["GCLOUD_PROJECT"]
        job = "asset-gcs-to-pubsub"
        inputFilePattern = "gs://{}/{}".format(str(event["bucket"]), str(event["name"]))
        outputTopic = os.environ["OUTPUT_TOPIC"]

        # inputFilePattern - The input file pattern to read from. For example, gs://bucket-name/files/*.json.
        # outputTopic - The Pub/Sub input topic to write to. The name must be in the format of projects/<project-id>/topics/<topic-name>.

        parameters = {
            "inputFilePattern": inputFilePattern,
            "outputTopic": outputTopic,
        }

        # tempLocation - path on GCS to store temp files generated during the dataflow job

        environment = {"tempLocation": temp_location}

        # launch dataflow job

        service = build("dataflow", "v1b3", cache_discovery=False)
        request = (
            service.projects()
            .locations()
            .templates()
            .launch(
                projectId=project,
                gcsPath=template,
                location="us-central1",
                body={
                    "jobName": job,
                    "parameters": parameters,
                    "environment": environment,
                },
            )
        )
        response = request.execute()
EOF
cat << EOF > requirements.txt
# Function dependencies
google-cloud-resource-manager==0.30.2
EOF
gcloud functions deploy ${DF_FUNCTION} \
  --runtime=python37 \
  --ingress-settings=internal-only \
  --timeout=540 \
  --trigger-event=google.storage.object.finalize \
  --trigger-resource=gs://${ASSET_BUCKET} \
  --entry-point=hello_gcs \
  --set-env-vars=GCS_TEMP_PATH=gs://${DF_TEMP_BUCKET}/,OUTPUT_TOPIC=${DF_BATCH_OUTPUT_TOPIC} \
  --region=${GCP_REGION}
```

##### Configure Cloud Scheduler

Configure the Cloud Scheduler to export an asset inventory every 24 hours.

```bash
gcloud scheduler jobs create pubsub ${CS_JOB_NAME} --schedule "0 0 * * *" --time-zone="Etc/UTC" --topic ${PUBSUB_TOPIC} --message-body "trigger" --location=${GCP_REGION}
```

##### Configure props.conf

You will need to add the following to the `[google:gcp:pubsub:message]` section of `$SPLUNK_HOME/etc/apps/Splunk_TA_google-cloudplatform/local/props.conf`:

```
TRANSFORMS-sourcetype_gcp_assets = sourcetype_gcp_assets
```

##### Configure transforms.conf

Similarly, the following addition is required in the `$SPLUNK_HOME/etc/apps/Splunk_TA_google-cloudplatform/local/transforms.conf` file:

```
[sourcetype_gcp_assets]
REGEX = \"asset_type\"\:
FORMAT = sourcetype::google:gcp:assets
DEST_KEY = MetaData:Sourcetype
```

Please note that this is in addition to the `props.conf` and `transforms.conf` [modifications required by the GCP Application Template](https://splunkbase.splunk.com/app/5404/#/details).

## Recommendation

In this blog, I've described three methods for ingesting cloud asset inventory data into Splunk. But which is "the best?" Like many things in cloud, there isn't one universally "right" answer.

If you're just getting started and just want to get the reports up and running, I recommend trying the "pull from bucket" method described in option #1. The number of moving pieces is minimal and it is relatively easy to setup. It's also dirt cheap in comparison to other approaches and will likely take you pretty far.

If you're already using Dataflow to stream Cloud Logging events into Splunk, then I recommend pursuing option #3. Customers usually find themselves turning to the Dataflow method for sending Cloud Logging events to Splunk as they scale out their infrastructure and need to ensure what they deploy is easily monitorable, horizontally-scalable, and fault-tolerant. Why not leverage that same infrastructure investment to deliver asset inventory data to Splunk?

If option #3 is not within your reach or you view it as "overkill," the "serverless push-to-Splunk" method described in option #2 may be preferable. It has some of the same fault-tolerant and horizontally scalable properties I praise when positioning Dataflow but without the cost of running a batch streaming Dataflow job. Keep in mind that neither Google or Splunk support can assist in the operation of this method, however. You could find yourself "on your own" should things go wrong -- if you're building a production pipeline, skip this method. If you're having fun in the lab, go for it.

## Conclusion

Whether you're just experimenting in a lab environment or building a production Dataflow pipeline, one of the methods described in this blog should have your cloud inventory ingest requirements covered. And once you've got that data into Splunk, you'll be generating dashboards and reports with the GCP Application Template in no time!

## Resources

* [GCP Application Template](https://splunkbase.splunk.com/app/5404/)
* [Splunk Add-on for Google Cloud Platform](https://splunkbase.splunk.com/app/3088/)
* [GCP Functions Library for Ingesting into Splunk](https://github.com/splunk/splunk-gcp-functions)
* [Deploying production-ready log exports to Splunk using Dataflow](https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow)
* [Pub/Sub to Splunk streaming Dataflow template](https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#cloudpubsubtosplunk)
* [Cloud Storage to Pub/Sub batch Dataflow template](https://cloud.google.com/dataflow/docs/guides/templates/provided-batch#gcstexttocloudpubsub)
